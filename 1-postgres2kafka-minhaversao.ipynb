{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo os dados do Postgres e publicando no Kafka\n",
    "\n",
    "Esta primeira etapa simula um ambiente de um banco de dados ERP, onde é inviável utilizar ferramentas como Sqoop para integração de dados, devido a regras de segurança da empresa que hospeda o ERP. Por isso, os dados de clientes, pedidos e itens de pedidos são lidos do Postgres e enviados para um tópico no Kafka. \n",
    "\n",
    "Em um cenário real, geralmente, existe um sistema integrador que lê os dados do Postgres no ambiente do ERP e envia para uma API REST dentro da \"nossa\" plataforma Big Data. A API REST recebe os dados e escreve no tópico no Kafka. Este *Hands on* possui um integrador, que apresentamos neste notebook, que faz o papel do sistema integrador no ambiente do ERP na leitura dos dados e da API REST na publicação destes dados no tópico do Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente\n",
    "\n",
    "O código abaixo adiciona a **raiz** do projeto, que contém códigos e dados necessários para o \"Hands on\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/bigdata/jupyterhub'\n",
    "\n",
    "import sys\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho de código abaixo prepara o ambiente, carregando códigos auxiliares e dados de configuração.O código disponível no pacote *commom.utils* na classe *DataframeUtils* contém vários métodos que facilitam a leitura e escrita dos dados do Postgres. A classe *DataframeUtils* também inicia uma instância do Apache Spark com o Delta Lake integrado ao Spark.\n",
    "\n",
    "Já o arquivo *config.yaml* tem os dados de acesso ao Postgres e Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "from common.utils import DataframeUtils\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "config = yaml.safe_load(open('../config.yaml'))\n",
    "dfu = DataframeUtils(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escrita no Apache Kafka\n",
    "\n",
    "O método *send_to_kafka* tem como objetivo escrever os dados no Kafka. Neste tutorial, os dados de clientes, pedidos e itens de pedidos serão obtidos do Postgres e escritos no Kafka. O método recebe uma partição do Dataframe do Spark (*iterator*) que contém vários itens a serem escritos no Kafka.\n",
    "\n",
    "Para escrever no Kafka, devemos criar um *Producer* que aponta para os servers bootstrap do Kafka (*bootstrap.servers*). O host do servidor boostrap do Kafka está definido no arquivo *config.yaml* e é carregado em *dfu.config()*. Os itens dentro do *iterator* são lidos e escritos no Kafka em blocos de 10000 com o método *flush*. \n",
    "\n",
    "A linha abaixo prepara a mensagem do Kafka com os dados que serão escritos, passando como argumento o tópico do Kafka, a chave que identifica a mensagem e o valor da mensagem. A chave e valor vem dentro de cada item da partição do Dataframe Spark (*iterator*).\n",
    "```\n",
    "p.produce(topic=topic, key=str(item.key), value=item.value)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka = dfu.config()['kafka']['host']\n",
    "def send_to_kafka(iterator, topic):\n",
    "    from confluent_kafka import Producer\n",
    "\n",
    "    p = Producer({'bootstrap.servers': kafka})\n",
    "\n",
    "    count = 0\n",
    "    for item in iterator:\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} sending item: {item}')\n",
    "\n",
    "        p.produce(topic=topic, key=str(item.key), value=item.value)\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            p.flush()\n",
    "\n",
    "    p.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados do Postgres com Apache Spark\n",
    "O método *process* facilita a leitura dos dados do Postgres com o uso do Apache Spark. O método lê os dados da tabela do Postgres (*table*) e armazena no Dataframe *df* (lembrando que é uma operação *lazy*). A variável *dfu* é uma instância da classe *DataframeUtils*, que contém os métodos auxiliares de leitura e escrita no Postgres utilizando Spark.\n",
    "\n",
    "Os dados são enviados para o Kafka no formato JSON (*F.to_json*). Por isso, o Dataframe *df* é alterado para ter apenas duas colunas:\n",
    "- **key**: uma chave única para a mensagem com o método *monotonically_increasing_id*.\n",
    "- **value**: dados da tabela do Postgres transformados para o formato JSON com o método *F.to_json*\n",
    "\n",
    "Os dados de cada partição do Dataframe *df* são então enviados para o Kafka. A divisão em partições reduz o volume de dados a ser recuperado de *df* em cada etapa de envio par ao Kafka no método *send_to_kafka*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(table, topic):\n",
    "    df = dfu.read_pg(table)\n",
    "    \n",
    "    df = df \\\n",
    "        .select(F.struct('*').alias('json_col')) \\\n",
    "        .select(F.monotonically_increasing_id().alias('key'), F.to_json('json_col').alias('value'))\n",
    "\n",
    "    df.foreachPartition(lambda partition: send_to_kafka(partition, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura do Postgres e escrita no Kafka\n",
    "\n",
    "Chegou a hora de usar os métodos auxiliares para ler os dados do Postgres com Apache Spark e escrever no Kafka no formato JSON. Primeiro, iremos escrever os dados da tabela *clientes* e depois a tabela *pedidos*. O primeiro argumento do método *process* indica o nome da tabela do Postgres e o segundo argumento o nome do tópico do Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 23.26 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "process('clientes', 'clientes')\n",
    "process('pedidos', 'pedidos')\n",
    "print(f'Took {time.time() - start:.2f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício\n",
    "\n",
    "Agora é com **você**. Neste exercício você irá utilizar os códigos apresentados acima como exemplo para fazer a **leitura da tabela de itens de pedidos do Postgres e escrever no Kafka**.\n",
    "\n",
    "**Não será permitido utilizar as funções process(), send_to_kafka() e read_pg()**. Você deverá construir seu próprio código utilizando os dados fornecidos abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host do boostrap server do Kafka\n",
    "kafka_host = dfu.config()['kafka']['host']\n",
    "kafka_topic = 'itens'\n",
    "\n",
    "# Spark session\n",
    "spark = dfu.spark()\n",
    "\n",
    "# Dados do Postgres\n",
    "dbtable = 'itens'\n",
    "pguser = dfu.pguser()\n",
    "pgpass = dfu.pgpass()\n",
    "jdbc_url = dfu.pgurl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos fazer a leitura dos dados da tabela **itens** do Postgres que contém os dados de itens de pedidos, utilizando o Apache Spark. Neste passo, iremos fazer juntos para você entender a ideia do exercício. Como não é possível utilizar a função *read_pg* do pacote utils, temos que construir o código de leitura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_itens = spark.read. \\\n",
    "            format('jdbc'). \\\n",
    "            option('url', jdbc_url). \\\n",
    "            option('driver', 'org.postgresql.Driver'). \\\n",
    "            option('dbtable', dbtable). \\\n",
    "            option('user', pguser). \\\n",
    "            option('password', pgpass). \\\n",
    "            load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo é fazer uma transformação no Dataframe *df_itens* para que os dados fiquem no formato JSON para serem enviados para o Kafka. Cada tupla terá a chave **única** armazenada na coluna **key** e o objeto JSON na coluna **value**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente o código abaixo e continue ele\n",
    "df_itens = df_itens.select(F.struct('*').alias('json_col')) \\\n",
    "        .select(F.monotonically_increasing_id().alias('key'), F.to_json('json_col').alias('value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos percorrer as partições do Dataframe para escrever as tuplas de *df_itens* no Kafka. No código abaixo, você deverá criar a função **send_itens** que envia as tuplas do Dataframe **df_itens** para o Kafka. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_itens(iterator, topic):\n",
    "    from confluent_kafka import Producer\n",
    "    p = Producer({'bootstrap.servers': kafka})\n",
    "    count = 0\n",
    "    for item in iterator:\n",
    "        if count % 1000 == 0:\n",
    "            print('*** sending item: {}'.format(item))\n",
    "        p.produce(topic=topic, key=str(item.key), value=item.value)\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            p.flush()\n",
    "\n",
    "    p.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora você irá percorrer as partições do Dataframe **df_itens**, enviando os dados para o Kafka, chamando a função **send_itens()** que você criou na célula anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coloque seu código aqui\n",
    "df_itens.foreachPartition(lambda partition: send_itens(partition, kafka_topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parabéns**! Chegamos ao **fim** da primeira etapa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
