{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movendo os dados do Kafka para o Delta\n",
    "\n",
    "Na primeira etapa, você enviou os dados de clientes, pedidos e itens de pedidos do Postgres para o Kafka. O Apache Kafka armazena as mensagens em estrutura de tópicos, de forma confivel e escalável. Agora, na segunda etapa, iremos ler os dados do Kafka e armazenar na tabela bronze no Delta Lake. A tabela bronze armazena os dados brutos, sem nenhuma transformação, para que posteriormente seja possível aplicar qualquer transformação sobre os dados, sem perder a origem da informação. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente\n",
    "\n",
    "O código abaixo adiciona a **raiz** do projeto, que contém códigos e dados necessários para o \"Hands on\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/bigdata/jupyterhub'\n",
    "\n",
    "import sys\n",
    "sys.path.append(root)\n",
    "\n",
    "wd = '/delta'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho de código abaixo prepara o ambiente, carregando códigos auxiliares e dados de configuração.O código disponível no pacote *commom.utils* na classe *DataframeUtils* contém vários métodos que facilitam a leitura e escrita dos dados do Postgres. A classe *DataframeUtils* também inicia uma instância do Apache Spark com o Delta Lake integrado ao Spark.\n",
    "\n",
    "Já o arquivo *config.yaml* tem os dados de acesso ao Postgres e Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "from common.utils import DataframeUtils\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "config = yaml.safe_load(open('../config.yaml'))\n",
    "dfu = DataframeUtils(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados do Kafka e escrita no Delta Lake\n",
    "\n",
    "O método *process* facilita a leitura dos dados do Kafka utilizando o Apache Spark. Com o trecho do código abaixo, os dados são lidos do tópico do Kafka e armazenados no Dataframe **df** (de forma lazy). Para ler os dados Kafka, tem que passar como argumento o *format('kafka')* para indicar que é uma leitura do Kafka, o host do servidor boostrap, o tópico que irá ler as mensagens e o offset para indicar a partir de onde irá ler as mensagens.\n",
    "\n",
    "```\n",
    "df = dfu.spark() \\\n",
    "        .readStream \\\n",
    "        .format('kafka') \\\n",
    "        .option('kafka.bootstrap.servers', kafka) \\\n",
    "        .option('subscribe', topic) \\\n",
    "        .option('startingOffsets', 'earliest') \\\n",
    "        .option(\"maxOffsetsPerTrigger\", max_triggers) \\\n",
    "        .load()\n",
    "```\n",
    "\n",
    "Os dados lidos do Kafka estão no formato **JSON** e, por isso, tem que ser transformados para o formato tabular para poder inserir no formato Delta. Por isso, a chave **key** do Kafka torna uma coluna do Delta, com nome *key*, que identifica a tupla e o conteúdo da mensagem em **value** é escrito em outra coluna com o nome *value*. Como os dados do Kafka chegam no formato de *stream*, eles são armazenados utilizando a função **writeStream**, que vai recebendo os dados do Kafka, fazendo a transformação para o formato tabular e armazenando na tabela Gold. O trecho `option('mergeSchema', 'true')` indica que se houver mudanças no esquema de dados, será feito um \"merge\" dos esquemas no Delta Lake. O Delta Lake armazena o histórico de alterações dos dados e permite indicar o diretório de checkpoint através da opção `.option('checkpointLocation', checkpoint_dir)`.\n",
    "\n",
    "```\n",
    "df\n",
    " .withColumn('key', F.col('key').cast('string'))\n",
    " .withColumn('value', F.col('value').cast('string'))\n",
    " .writeStream\n",
    " .option('mergeSchema', 'true')\n",
    " .format('delta')\n",
    " .outputMode('append')\n",
    " .option('checkpointLocation', checkpoint_dir)\n",
    " .start(output_dir)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(topic, output_dir, checkpoint_dir, kafka, max_triggers:int=1000):\n",
    "    df = dfu.spark() \\\n",
    "        .readStream \\\n",
    "        .format('kafka') \\\n",
    "        .option('kafka.bootstrap.servers', kafka) \\\n",
    "        .option('subscribe', topic) \\\n",
    "        .option('startingOffsets', 'earliest') \\\n",
    "        .load()\n",
    "\n",
    "    return (df\n",
    "     .withColumn('key', F.col('key').cast('string'))\n",
    "     .withColumn('value', F.col('value').cast('string'))\n",
    "     .writeStream\n",
    "     .option('mergeSchema', 'true')\n",
    "     .format('delta')\n",
    "     .outputMode('append')\n",
    "     .option('checkpointLocation', checkpoint_dir)\n",
    "     .start(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos próximos três trechos de código, serão escritos os dados de cliente em *'/delta/data/clientes-bronze'* e os dados de pedidos em *'/delta/data/pedidos-bronze'*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka = dfu.config()['kafka']['host']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc = process('clientes', wd + '/data/clientes-bronze', wd + '/checkpoints/clientes-checkpoint', kafka, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi = process('itens', wd + '/data/itens-bronze', wd + '/checkpoints/itens-checkpoint', kafka, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício\n",
    "\n",
    "Agora é com **você**. Neste exercício você irá utilizar os códigos apresentados acima como exemplo para fazer a **leitura dos dados no Kafka e escrita dos dados com o Delta Lake**.\n",
    "\n",
    "**Não será permitido utilizar a função process()**. Você deverá construir seu próprio código utilizando os dados fornecidos abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = dfu.spark()\n",
    "topic = 'itens'\n",
    "output_dir = wd + '/data/itens-bronze'\n",
    "checkpoint_dir = wd + '/checkpoints/itens-checkpoint'\n",
    "max_triggers=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No trecho de código abaixo, você deverá ler o fluxo de dados do tópico do Kafka e armazenar no Dataframe Spark **df_itens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_itens = spark \\\n",
    "        .readStream \\\n",
    "        .format('kafka') \\\n",
    "        .option('kafka.bootstrap.servers', kafka) \\\n",
    "        .option('subscribe', topic) \\\n",
    "        .option('startingOffsets', 'earliest') \\\n",
    "        .option(\"maxOffsetsPerTrigger\", max_triggers) \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No trecho de código abaixo, você deverá escrever o fluxo de dados que está no Dataframe *df_itens* no formato **delta**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi = df_itens \\\n",
    "     .withColumn('key', F.col('key').cast('string')) \\\n",
    "     .withColumn('value', F.col('value').cast('string')) \\\n",
    "     .writeStream \\\n",
    "     .option('mergeSchema', 'true') \\\n",
    "     .format('delta') \\\n",
    "     .outputMode('append') \\\n",
    "     .option('checkpointLocation', checkpoint_dir) \\\n",
    "     .start(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitorando o fluxo de dados\n",
    "Os códigos abaixo monitoram o fluxo de dados que está sendo processado nos Dataframes Spark e escritos no formato Delta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsp.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsi.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsp.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfu.print_streaming_chart(dsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfu.print_streaming_chart(dsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfu.print_streaming_chart(dsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
