{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo os dados da Delta Table\n",
    "\n",
    "No passo anterior você iniciou a leitura dos dados do Kafka para escrever nas Delta Tables, com o código abaixo é possível visualizar a evolução dos dados.\n",
    "Na etapa anterior você utilizou o *Spark Streamings* para ler os itens em buckets de 1.000 e escrever no HDFS, agora vamos ver se os dados já foram todos escritos.\n",
    "\n",
    "Lembrando, existem **15.000** clientes, **310.976** pedidos e **2.410.176** itens de pedido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente\n",
    "\n",
    "O código abaixo adiciona a **raiz** do projeto, que contém códigos e dados necessários para o \"Hands on\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/bigdata/jupyterhub'\n",
    "\n",
    "import sys\n",
    "sys.path.append(root)\n",
    "\n",
    "wd = '/delta'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho de código abaixo prepara o ambiente, carregando códigos auxiliares e dados de configuração.O código disponível no pacote *commom.utils* na classe *DataframeUtils* contém vários métodos que facilitam a leitura e escrita dos dados do Postgres. A classe *DataframeUtils* também inicia uma instância do Apache Spark com o Delta Lake integrado ao Spark.\n",
    "\n",
    "Já o arquivo *config.yaml* tem os dados de acesso ao Postgres e Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "from common.utils import DataframeUtils\n",
    "\n",
    "config = yaml.safe_load(open('../config.yaml'))\n",
    "dfu = DataframeUtils(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho abaixo cria o Dataframe Spark de pedidos apontando para o diretório do HDFS onde o Spark Streaming está escrevendo.\n",
    "Lembre-se, essa operação no Spark é *lazy*, portanto o código abaixo cria apenas o *link* entre o Spark e os dados no HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos = dfu. \\\n",
    "  spark(). \\\n",
    "  read. \\\n",
    "  format('delta'). \\\n",
    "  load(f'{wd}/data/pedidos-bronze')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho abaixo obtém a quantidade de itens no Dataframe, execute-o algumas vezes em intervalos de alguns segundos e observe o valor sendo incrementado.\n",
    "\n",
    "Veja que não é necessário recriar o Dataframe para que o valor seja atualizado, o Spark já cuida dessa parte para você.\n",
    "O valor final deverá ser **310.976**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310976"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pedidos.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho abaixo cria o Dataframe de clientes exatamente da mesma forma que o de pedidos, alterando apenas o diretório no HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes = dfu. \\\n",
    "  spark(). \\\n",
    "  read. \\\n",
    "  format('delta'). \\\n",
    "  load(f'{wd}/data/clientes-bronze')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quantidade de clientes é muito pequena, portanto quando você executar o trecho abaixo já terá finalizado a escrita dos **15.000** clientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho abaixo cria uma *temp view* do Spark para que possamos executar uma consulta SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes.createOrReplaceTempView('clientes_bronze')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SQL abaixo é para verificar que não existem clientes repetidos.\n",
    "\n",
    "Para isso realizamos um *group_by* na chave (key) e adicionamos um *having count > 1*.\n",
    "O resultado abaixo deverá retornar uma lista vazia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>key</th><th>count(value)</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+------------+\n",
       "|key|count(value)|\n",
       "+---+------------+\n",
       "+---+------------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfu.spark().sql(\"\"\"\n",
    "select key, count(value)\n",
    "from clientes_bronze\n",
    "group by 1\n",
    "having count(value) > 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício\n",
    "\n",
    "Quer saber quantos itens de pedido foram todos escritos?\n",
    "Adicione abaixo o código responsável por criar o Dataframe de itens de pedido e imprimir a quantidade de itens do Dataframe.\n",
    "\n",
    "O diretório no HDFS é o **/delta/data/itens-bronze** e o count final deverá ser **2.410.176**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete o código para ler os dados de itens de pedido\n",
    "df_itens = dfu. \\\n",
    "  spark(). \\\n",
    "  read. \\\n",
    "  format('delta'). \\\n",
    "  load(f'{wd}/data/itens-bronze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2410176"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adicione o código para imprimir a quantidade de itens do Dataframe\n",
    "df_itens.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
